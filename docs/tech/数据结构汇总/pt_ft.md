---
title: pt_ft
date: 2025-07-02  
---

#### **Part 1: 继续预训练 (Continued Pre-training, PT)**
**1. 核心目标：知识灌输 (Knowledge Infusion)**

+ **一句话概括**：让模型成为一个“特定领域的书呆子”。
+ **详细描述**：继续预训练的目标，是向一个已经具备通用知识的LLM（如Qwen），注入**大量特定领域的专业知识和语言风格**。例如，让它学习海量的医学文献、法律文书或公司内部的技术文档。这个过程并不教模型如何“对话”，而是让它熟悉这个领域的词汇、概念、实体关系和叙事方式，使其语言模型的基础概率分布更贴近该领域。

**2. 数据结构**

+ **格式**：极其简单，就是**纯文本**。通常是一个包含大量文本段落的 `.txt` 文件，或者是一个每行都是一个JSON对象、且对象中只有一个`"text"`字段的 `.jsonl` 文件。

**示例 (`my_domain_corpus.jsonl`)**：

+ JSON

```plain
{"text": "本文将深入探讨心脏瓣膜置换手术的最新进展，特别是在生物材料和微创技术方面的应用。根据2024年《柳叶刀》杂志的研究..."}
{"text": "在起草股权转让协议时，必须明确定义转让的标的、价格、支付方式以及双方的权利和义务。尤其要注意竞业限制条款的合法性..."}
{"text": "我们的XYZ型涡轮增压发动机采用了最新的可变截面技术，通过优化进气歧管的压力波动，实现了在低转速下更高的扭矩输出..."}
```

+ **核心要求**：数据**质量**和**领域相关性**至关重要。数据应该是干净、无格式噪音的长篇文章、段落或句子。

**3. 策略与关键参数**

+ **训练任务**：与原始预训练一样，是“**下一词预测**”（Next Token Prediction）。模型读取一段文本，并学习预测下一个最可能出现的词。
+ **策略**：
    - 在通用模型（如Qwen）上进行训练。
    - 通常需要较大的数据量（至少百万级别Token，越多越好）才能看到明显效果。
+ **关键参数**：
    - `learning_rate` (学习率)：通常设置得**非常小**（例如 `5e-6` 到 `2e-5`）。因为我们是在一个已经很强大的模型上进行微调，学习率过高会破坏模型原有的通用能力，导致“灾难性遗忘”。
    - `epochs` (训练轮次)：通常只需要1-3个epoch。因为数据量很大，过多的轮次容易导致模型对领域数据过拟合。

---

#### **Part 2: 指令监督微调 (Supervised Fine-tuning, SFT/FT)**
**1. 核心目标：行为对齐 (Behavior Alignment)**

+ **一句话概括**：将一个“书呆子”训练成一个“乐于助人、听得懂指令的对话伙伴”。
+ **详细描述**：SFT的目标不是教模型新知识，而是教会它**如何使用已有的知识，来遵循人类的指令格式进行有用的、安全的回答**。它学习的是一种“行为模式”或“对话范式”。

**2. 数据结构**

+ **格式**：高质量的、结构化的“指令-回答”数据对。目前最主流的是**对话格式**，通常是一个包含多轮对话历史的JSON对象列表。

**示例 (`my_sft_dataset.jsonl`)**：

+ JSON

```plain
{"conversation": [{"from": "human", "value": "请解释一下什么是“心脏瓣膜置换手术”？"}, {"from": "gpt", "value": "心脏瓣膜置换手术是一种用于修复或替换受损或病变心脏瓣膜的外科手术。当瓣膜无法正常打开（狭窄）或关闭（反流）时，就需要进行这种手术..."}]}
{"conversation": [{"from": "human", "value": "根据这份财报，总结一下公司的核心风险。财报摘要：[...财报内容...]"}, {"from": "gpt", "value": "根据您提供的财报摘要，公司的核心风险主要有三点：1. 市场竞争加剧导致毛利率下降... 2. 对单一供应商的过度依赖... 3. 国际贸易政策的不确定性..."}]}
```

+ **核心要求**：**指令的多样性**至关重要。需要覆盖各种类型的指令（开放式问答、总结、翻译、代码、角色扮演等），以培养模型的泛化能力。回答的**质量**（准确、有用、安全）直接决定了微调后模型的能力上限。

**3. 策略与关键参数**

+ **训练任务**：同样是“下一词预测”，但只计算和反向传播**模型回答部分（`gpt`角色）的loss**。模型的目标是学习在给定指令和历史对话后，生成最像“标准答案”的回答。
+ **策略**：
    - **全量微调 (Full FT)**：更新模型的所有参数。效果潜力最大，但需要巨大的算力（例如，微调Qwen2-72B需要多张A100/H100）。
    - **参数高效微调 (PEFT, 如LoRA/QLoRA)**：只更新一小部分“适配器”参数。极大地降低了硬件门槛，是目前社区和工业界微调的主流选择。
+ **关键参数**：
    - `learning_rate` (学习率)：通常比继续预训练**稍高一些**（例如 `1e-4` 到 `3e-4` 范围对于LoRA是常见的），因为我们是在训练新增的适配器，或者希望模型更快地适应新的行为模式。
    - `epochs` (训练轮次)：通常3个epoch是一个常见的基准，但需要通过监控验证集的loss来防止过拟合。

---

#### **终极策略：PT + FT 组合拳**
在资源允许的情况下，最强大的策略是将两者结合：

1. **第一步 (PT)**：先用海量的领域文档对基础模型（如Qwen）进行**继续预训练**，让它成为满腹经纶的“领域专家”。
2. **第二步 (FT)**：然后，用高质量的指令数据对第一步产出的“专家模型”进行**指令微调**，教会它如何利用满腹的专业知识，来与人进行高质量的对话和互动。

这个“先学知识，再学做人”的两阶段策略，是打造顶尖领域模型的黄金路径。

