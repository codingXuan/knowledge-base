---
title: 模型评估与迭代
---

**1. 指导原则：场景、数据与模型的匹配**

模型评估与迭代的第一步是基于业务的“顶层设计”。其核心原则是实现业务场景、可用数据和模型能力三者之间的最佳匹配。

+ **场景决定模型架构**：
    - **判别式任务**（如意图识别、情感分类）：优先考虑轻量级的BERT、RoBERTa等编码器（Encoder）模型，或传统的机器学习模型（如XGBoost），成本效益高。
    - **生成式任务**（如多轮对话、文案撰写、代码生成）：需要选择基于Transformer架构的生成式大语言模型（LLM），如GPT系列、Llama系列，或支持MoE（混合专家）架构的模型以提高效率。
+ **数据决定策略与模型规模**：
    - **数据量极少（< 1万条）**：**不建议进行微调**。微调很可能导致模型“过拟合”，忘记原有的通用能力。此时最佳策略是 **RAG（检索增强生成）**，将少量高质量数据作为上下文，通过Prompt工程来利用大模型的零样本（Zero-shot）或少样本（Few-shot）能力。
    - **数据量中等（10万-100万条）**：这是**微调的最佳区间**。可以选择一个合适尺寸的基础模型（如7B、13B，一般不超过30B）进行全量或部分参数微调（如LoRA）。目标是向模型注入领域知识和特定的任务范式。
    - **数据量巨大（千万级以上）**：具备了进行**持续预训练**（Continual Pre-training）甚至从零开始训练一个模型（Train from Scratch）的潜力。此时需要选择更大参数量的模型（如70B以上）来有效“吸收”海量数据中的知识。

**2. 核心环节：科学的量化评估体系**

迭代的基础是评估。没有评估，优化就无从谈起。一个科学的评估体系包含以下部分：

+ **A. 构建“黄金”评估集**：
    - 在项目初期，投入人力构建一个能反映核心业务场景、数据分布均衡、高质量的、规模适中（如1000条）的测试集。
    - **该数据集在迭代过程中保持不变**，作为衡量所有模型版本优劣的“恒定标尺”。
+ **B. 选择多维度的评估指标**：
    - **离线评估**：
        * **客观指标**：针对有标准答案的任务，使用精确率、召回率、F1等。
        * **模型评估**：利用`LLM-as-a-Judge`，让顶级LLM根据自定义的维度（如“遵循指令”、“逻辑清晰”、“富有创意”）对模型输出进行1-10分的打分。
    - **在线评估**：
        * **A/B 测试**：将新旧两个版本的模型同时部署，让它们服务于真实用户，对比点击率、转化率、满意度等真实业务指标。
        * **人工评估**：由内部的业务专家团队对模型的线上输出进行抽样和评级，这是最可靠但也成本最高的评估方式。

**3. 迭代过程：持续提纯的循环**

模型的迭代是一个螺旋式上升的、以数据为中心的闭环流程。

+ **定义**：迭代并非简单地用新数据重新训练，而是在一个已经表现良好的模型版本（`v1.0`）的基础上，利用新收集到的、高质量的数据（尤其是线上真实场景的“Bad Case”），进行**增量微调（Incremental Fine-tuning）**，从而得到一个更强的模型版本（`v1.1`）。
+ **循环流程**：
    1. **收集数据**：从线上应用、人工标注、数据增强等渠道收集新的高质量数据。
    2. **清洗与标注**：对新数据进行严格的清洗和标注。
    3. **增量微调**：在上一版最优模型的基础上，用新数据进行微调。
    4. **全面评估**：使用“黄金评估集”和多维度指标，科学地评估新模型是否真的比旧模型更好。
    5. **上线部署**：如果评估通过，将新模型部署上线（或进入A/B测试），替换旧模型。
    6. 回到第一步，开始新一轮的循环。

这个过程就像“提纯”一样，每一次循环，模型的领域知识更扎实，对特定任务的理解更深刻，从而不断逼近业务的理想状态。

