---
title: 部署与运维
---

**1. 核心原则：将CI/CD扩展到模型和数据**

LLMOps（大语言模型运维）的核心，是将传统DevOps中成熟的自动化、持续集成、持续部署（CI/CD）原则，扩展到包含**代码（Code）、模型（Model）和数据（Data）**三位一体的AI应用生命周期管理中。其目标是实现AI应用快速、可靠、可复现地交付与迭代。

**2. LLMOps的生命周期闭环**

一个完整的LLMOps流程是一个持续的循环：

1. **数据管理与处理 (CI for Data)**：自动化地收集、清洗、标注新的数据，并进行版本化管理。
2. **模型训练与微调 (CT for Models)**：当有新数据或新代码时，自动触发模型的微调或训练流程。
3. **模型评估与验证 (CI for Models)**：使用“黄金评估集”自动评估新生成的模型，只有当新模型的性能指标优于旧模型时，才允许进入部署环节。
4. **模型打包与部署 (CD for Models)**：将通过验证的模型、相关代码和配置打包成一个可部署的单元（如Docker镜像），并根据预设策略部署到生产环境。
5. **线上服务与监控**：模型上线后，对其性能、成本、效果和安全性进行全方位监控。
6. **反馈收集**：从线上监控和用户反馈中收集有价值的数据，并将其送回到第一步的数据管理环节，形成下一次迭代的基础。

**3. LLM应用的部署策略**

在执行部署（CD）环节时，为了保证服务的稳定性和平滑过渡，可以根据业务的重要性和对中断的容忍度，选择不同的部署策略：

+ **蓝绿部署 (Blue-Green Deployment)**：同时部署两个完全相同的生产环境（蓝色和绿色）。新版本的模型部署在“绿色”环境，经过充分测试后，将所有流量瞬间从“蓝色”环境切换到“绿色”环境。优点是切换速度快，回滚方便；缺点是需要双倍的硬件资源。
+ **金丝雀发布 (Canary Release)**：先将新版本模型部署到一小部分服务器上，并导入少量用户流量（例如5%）。在确认新版本没有问题后，逐步扩大流量比例，直到100%覆盖。这是目前互联网应用中最主流的平滑发布方式。
+ **停机维护 (Downtime Deployment)**：对于内部系统或用户容忍度高的场景，最简单的策略是提前发布公告，在业务低峰期（如凌晨）暂停服务，进行升级，完成后再恢复服务。

**4. 关键技术：推理优化与模型服务**

+ **服务框架**：必须使用**vLLM, TGI, Triton**等专为LLM设计的服务框架，它们通过**PagedAttention**和**持续批处理（Continuous Batching）**等技术，能将推理吞吐量提升数倍甚至数十倍。
+ **硬件与资源管理**：
    - **GPU**：LLM推理是典型的计算密集型和显存密集型任务，GPU是必需品。
    - **容器化**：使用**Docker**将模型、代码和依赖打包，保证环境一致性。
    - **编排**：使用**Kubernetes**配合GPU设备插件，对GPU资源进行统一的调度和管理。

**5. 全面的监控体系**

一个生产级的LLM应用，至少需要监控以下几个维度：

+ **系统层**：GPU利用率、显存占用、网络I/O、API延迟、QPS。
+ **成本层**：输入/输出Token数、API调用总成本、每个用户的平均成本。
+ **效果层**：记录Prompt/Response对，通过用户点赞/点踩、人工抽样或模型自评估来监控服务质量。
+ **安全层**：监控异常的输入模式，防范Prompt注入和数据泄露风险。

通过建立这样一套完整的LLMOps体系，我们才能确保AIGC应用在生产环境中不仅能跑起来，还能跑得好、跑得稳，并且能够持续地自我进化。

