<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.24" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.94" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <title>rlhf_dpo | 我的知识库</title><meta name="description" content="一个记录学习与思考的地方">
    <link rel="preload" href="/knowledge-base/assets/style-Dmo5Xz_6.css" as="style"><link rel="stylesheet" href="/knowledge-base/assets/style-Dmo5Xz_6.css">
    <link rel="modulepreload" href="/knowledge-base/assets/app-BTyvM1qB.js"><link rel="modulepreload" href="/knowledge-base/assets/rlhf_dpo.html-DdsWU3Ui.js"><link rel="modulepreload" href="/knowledge-base/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/knowledge-base/assets/index.html-B1TI0TPh.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-YlaUiq8M.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-C1Q2wXbF.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/knowledge-management.html-BusDN2Dr.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/retrospective.html-C_pfrhiZ.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/change-management.html-Bz-4K7LI.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/meeting-management.html-C4Fj2-Y-.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/scrum.html-a8KMqEhA.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/tracking.html-OqvaRQGP.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/estimation.html-DhmkW9tt.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/risk-management.html-CMcEJJ3D.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/stakeholder-analysis.html-BUISbCT8.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/wbs.html-CZoG0etR.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/dora-metrics.html-fXvB00y2.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/leadership.html-BVm570oY.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/okr.html-CY29UO0g.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/pm-pdm-collaboration.html-CZgwzcGN.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/AIGC应用层建设思路.html-B1U6wMHf.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/RAG建设.html-D4R6q0FA.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-BH4EXv3f.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/prompt工程.html-CkeAOHFK.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/业务流程控制-agent.html-iuyUkjfV.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/多agent组合.html-CvBNj1dv.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/安全、隐私与伦理.html-C5mt_f7f.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/微调策略.html-QzgxnfQy.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/模型评估与迭代.html-IZRUHxBT.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/训练策略.html-C1fs0OiH.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/部署与运维.html-BKCTiFRl.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/长文本与算力.html-DdSy3ezj.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-B_tdmLLF.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-D1qTy6RU.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/TGI.html-wHQYyHWg.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/vLLM.html-BI8STYwc.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-CPdkHpzW.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/pt_ft.html-13VkP2In.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/rag.html-Cd268djj.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/AutoGen.html-BL_PZNxu.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/CrewAI.html-Bp84yC2j.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-CYRy3PCy.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-86ckvDhp.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/DeepSpeed.html-C-wZvHLP.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/Hugging Face Trainer _ PEFT.html-XJsnrwIo.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/Llama-Alpaca-Factory.html-CsK9a0mz.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-xhEuudls.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-BhnW8UDT.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/ChromaDB _ Weaviate.html-BQ1bkzPR.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-5eGEo5WG.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/faiss库.html-4XWhKTom.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/milvus.html-BUUpvVaO.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-Dh7Ucp6L.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/文本分割器-Text Splitters.html-BKTqeRA7.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/文档加载器-Document Loaders.html-Bh05c2Hd.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/BGE模型.html-DRhyv1YM.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/M3E模型.html-Bo4zPtua.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-Bn7Tas_E.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/text-embedding-ada-002模型.html-BFCg7b0o.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/LangChain.html-6Tmc7oTb.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/LlamaIndex.html-Di9tStnQ.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-C99XN3N4.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/Hugging Face Transformers.html-D7eBVZs1.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-Dk_wtOkn.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/Pytorch.html-LkNbfJbZ.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-BvbIasc5.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/TensorFlow_Keras.html-vE9pkSmq.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/404.html-LBivraXL.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-DJAoOdIS.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-CIeYcN5l.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-CwJh0Ssp.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-COHYLuT-.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-DrSbb9qZ.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-DPnvmCU3.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-D8h5xyC6.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-CdmCy2b6.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/index.html-xAi66OQG.js" as="script"><link rel="prefetch" href="/knowledge-base/assets/photoswipe.esm-CKV1Bsxh.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/knowledge-base/" aria-label="带我回家"><!----><!----><span class="vp-site-name">我的知识库</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/knowledge-base/" aria-label="首页"><!--[--><i class="vp-icon fas fa-home" sizing="height"></i><!--]-->首页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/knowledge-base/management/" aria-label="管理随笔"><!--[--><i class="vp-icon fas fa-people-roof" sizing="height"></i><!--]-->管理随笔<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/knowledge-base/tech/" aria-label="技术随笔"><!--[--><i class="vp-icon fas fa-code" sizing="height"></i><!--]-->技术随笔<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!----><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><i class="vp-icon fas fa-folder fa-fw" sizing="both"></i><span class="vp-sidebar-title">AIGC应用层建设思路</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><i class="vp-icon fas fa-folder fa-fw" sizing="both"></i><span class="vp-sidebar-title">AIGC框架详解</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><i class="vp-icon fas fa-folder fa-fw" sizing="both"></i><span class="vp-sidebar-title">推理服务与部署</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><i class="vp-icon fas fa-folder fa-fw" sizing="both"></i><span class="vp-sidebar-title">数据结构汇总</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/knowledge-base/tech/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB/rag.html" aria-label="rag"><!--[--><i class="vp-icon fas fa-file-lines fa-fw" sizing="both"></i><!--]-->rag<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/knowledge-base/tech/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB/pt_ft.html" aria-label="pt_ft"><!--[--><i class="vp-icon fas fa-file-lines fa-fw" sizing="both"></i><!--]-->pt_ft<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/knowledge-base/tech/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB/rlhf_dpo.html" aria-label="rlhf_dpo"><!--[--><i class="vp-icon fas fa-file-lines fa-fw" sizing="both"></i><!--]-->rlhf_dpo<!----></a></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->rlhf_dpo</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">codingXuan</span></span><span property="author" content="codingXuan"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025/7/2</span><meta property="datePublished" content="2025-07-02T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 5 分钟</span><meta property="timeRequired" content="PT5M"></span><!----><!----></div><hr></div><!----><div class="" vp-content><!----><div id="markdown-content"><h4 id="_1-核心目标-对齐-alignment-——让模型更像-人" tabindex="-1"><a class="header-anchor" href="#_1-核心目标-对齐-alignment-——让模型更像-人"><span><strong>1. 核心目标：对齐（Alignment）——让模型更像“人”</strong></span></a></h4><p>在完成了**继续预训练（PT，学习知识）<strong>和</strong>指令微调（SFT，学习遵循指令）**之后，模型已经具备了强大的能力。但此时的模型可能还存在一些问题：</p><ul><li>它可能会生成一些无害但无用的“废话”。</li><li>它的回答可能过于“机械”，缺乏真诚和共情。</li><li>在面对模棱两可的问题时，它不知道哪种风格的回答更受人类欢迎。</li></ul><p><strong>RLHF/DPO</strong>的目标，就是解决这个“好不好”的问题。它通过学习人类的<strong>偏好（Preference）数据，将模型的价值观和行为准则，与人类的期望进行对齐（Alignment）</strong>，使其变得更<strong>有用（Helpful）、诚实（Honest）和无害（Harmless）</strong>。</p><h4 id="_2-经典的rlhf三阶段流程" tabindex="-1"><a class="header-anchor" href="#_2-经典的rlhf三阶段流程"><span><strong>2. 经典的RLHF三阶段流程</strong></span></a></h4><p>传统的RLHF是一个复杂的多阶段过程，著名的ChatGPT-3.5就是通过这个流程训练出来的。</p><ul><li><strong>第一阶段：监督微调（SFT）</strong><ul><li>（我们已在上一篇文档中详细讨论）</li><li>这一步的产物是一个已经能听懂指令的基础对话模型。</li></ul></li><li><strong>第二阶段：训练奖励模型（Reward Model, RM）</strong><ul><li><strong>数据准备</strong>：这是RLHF最耗费人力的地方。 <ol><li>让SFT模型对同一个Prompt生成多个不同的回答（例如A, B, C, D）。</li><li>由人工标注员根据一系列标准（如哪个更真实、更友好、更符合指令），对这些回答进行<strong>排序</strong>（例如，<code>D &gt; B &gt; A &gt; C</code>）。</li><li>这样就构成了一份<strong>人类偏好数据集</strong>。</li></ol></li><li><strong>模型训练</strong>： <ul><li>奖励模型（RM）的结构通常与基础模型类似，但其任务不是生成文本，而是<strong>输入一段文本（Prompt+Answer），输出一个单一的分数（Scalar Reward）</strong>。</li><li>我们用偏好数据来训练这个RM，目标是让RM打出的分数，能够尽可能地拟合人类的排序结果（即，<code>RM(D) &gt; RM(B) &gt; RM(A) &gt; RM(C)</code>）。</li><li>训练完成后，我们就拥有了一个可以模仿人类偏好进行打分的“<strong>奖惩模型</strong>”。</li></ul></li></ul></li><li><strong>第三阶段：强化学习（Reinforcement Learning, RL）</strong><ul><li><strong>流程</strong>： <ol><li>将SFT模型作为“策略（Policy）”，让它针对一个新的Prompt生成一个回答。</li><li>将这个“（Prompt，回答）”对送入第二阶段训练好的<strong>奖励模型（RM）</strong>，得到一个奖励分数。</li><li>这个分数被用作<strong>强化学习的“奖励信号”</strong>。</li><li>使用**PPO（Proximal Policy Optimization）**等强化学习算法，根据这个奖励信号，来微调SFT模型的参数。</li></ol></li><li><strong>目标</strong>：不断调整模型，使其生成的回答能够获得越来越高的奖励分数，从而让模型的行为越来越符合人类的偏好。</li></ul></li></ul><h4 id="_3-rlhf的挑战与dpo的兴起" tabindex="-1"><a class="header-anchor" href="#_3-rlhf的挑战与dpo的兴起"><span><strong>3. RLHF的挑战与DPO的兴起</strong></span></a></h4><p>RLHF虽然开创了对齐技术的先河，但其实施起来非常困难：</p><ul><li><strong>流程复杂</strong>：需要训练两个模型（SFT模型和RM），并涉及复杂的强化学习环节。</li><li><strong>训练不稳定</strong>：强化学习的训练过程非常不稳定，对超参数极其敏感。</li><li><strong>资源消耗大</strong>：整个流程需要耗费大量的计算资源。</li></ul><p>为了解决这些问题，**DPO（Direct Preference Optimization，直接偏好优化）**应运而生，并迅速成为当前SOTA（State-of-the-art）模型（如Llama-3, Qwen2）的主流对齐技术。</p><h4 id="_4-dpo-更简单、更直接的偏好对齐" tabindex="-1"><a class="header-anchor" href="#_4-dpo-更简单、更直接的偏好对齐"><span><strong>4. DPO：更简单、更直接的偏好对齐</strong></span></a></h4><p><strong>A. 核心思想</strong> DPO巧妙地证明了，我们可以<strong>跳过“训练奖励模型”和“强化学习”这两个复杂的步骤</strong>，直接在一个步骤内，实现与RLHF同样甚至更好的效果。</p><p><strong>B. DPO的工作流程</strong></p><ol><li><strong>数据准备</strong>：与RLHF类似，同样需要一份人类偏好数据集。但我们不需要完整的排序，只需要成对的比较数据，即对于一个Prompt，哪个回答是**“更优的”（Chosen）<strong>，哪个是</strong>“更差的”（Rejected）**。</li><li><strong>直接优化</strong>：DPO设计了一个特殊的损失函数。在训练时，它会直接对SFT模型进行微调，目标是： <ul><li><strong>增大</strong>模型生成“Chosen”回答的概率。</li><li><strong>减小</strong>模型生成“Rejected”回答的概率。 它将“学习奖励”这一隐式过程，直接转化成了“学习偏好数据”这一监督学习过程。</li></ul></li></ol><p><strong>C. DPO的数据结构</strong> DPO的数据格式非常直观，通常是一个包含三元组的JSONL文件：</p><div class="language-plain line-numbers-mode" data-highlighter="shiki" data-ext="plain" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-plain"><span class="line"><span>{</span></span>
<span class="line"><span>  &quot;prompt&quot;: &quot;请用一句话介绍一下北京。&quot;,</span></span>
<span class="line"><span>  &quot;chosen&quot;: &quot;北京是中国的首都，一座拥有三千年历史的文化名城。&quot;,</span></span>
<span class="line"><span>  &quot;rejected&quot;: &quot;北京，首都。&quot;</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>  &quot;prompt&quot;: &quot;如何学习AI？&quot;,</span></span>
<span class="line"><span>  &quot;chosen&quot;: &quot;学习AI可以从基础的数学和编程知识开始，然后系统地学习机器学习、深度学习等核心课程，并通过实践项目来巩固知识。&quot;,</span></span>
<span class="line"><span>  &quot;rejected&quot;: &quot;学习AI，就去学呗。&quot;</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>D. DPO的优势</strong></p><ul><li><strong>流程简单</strong>：只需在SFT模型上，用偏好数据再进行一次微调即可，无需训练独立的奖励模型，也无需复杂的强化学习。</li><li><strong>训练稳定</strong>：本质上是一次监督学习，比RL训练要稳定得多，更容易实现和调参。</li><li><strong>效果出色</strong>：在多项评测中，DPO被证明可以达到甚至超越传统RLHF的效果。</li></ul><p><strong>总结</strong>：<strong>RLHF</strong>是理解大模型“对齐”思想的<strong>奠基石</strong>，它开创性地引入了人类偏好来指导模型优化。而<strong>DPO</strong>则是这一思想的<strong>现代化、轻量化、高效化的继承者</strong>。在自己的项目中，如果需要进行偏好对齐，<strong>DPO是当前更推荐、更具实践性的技术选择</strong>。它能以更低的成本和更稳定的流程，让模型变得更“聪明”也更“懂事”。</p></div><!----><!----><!----></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">最近更新</span><time class="vp-meta-info" datetime="2025-07-30T02:46:26.000Z" data-allow-mismatch>2025/7/30 10:46</time></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 34129858+codingXuan@users.noreply.github.com">codingXuan</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/knowledge-base/tech/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB/pt_ft.html" aria-label="pt_ft"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><i class="vp-icon fas fa-file-lines" sizing="height"></i>pt_ft</div></a><!----></nav><!----><!----><!--]--></main><!--]--><!----></div><!--]--><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/knowledge-base/assets/app-BTyvM1qB.js" defer></script>
  </body>
</html>
