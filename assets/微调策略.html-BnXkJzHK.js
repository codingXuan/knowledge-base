import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as e,o as i}from"./app-Cadg2bap.js";const l={};function t(r,s){return i(),a("div",null,s[0]||(s[0]=[e(`<h4 id="_1-微调的目标与定位" tabindex="-1"><a class="header-anchor" href="#_1-微调的目标与定位"><span><strong>1. 微调的目标与定位</strong></span></a></h4><p>在上一篇文章中，我分享了对微调的理解。微调是在预训练模型的基础上，针对垂直领域进行特定任务的训练，或者可以理解为使用自身数据来进一步优化模型，使其更好地适应特定业务场景。这个过程的核心在于通过调整一系列关键参数和策略，来引导模型学习新的知识和行为范式。</p><h4 id="_2-微调策略的核心支柱" tabindex="-1"><a class="header-anchor" href="#_2-微调策略的核心支柱"><span><strong>2. 微调策略的核心支柱</strong></span></a></h4><p>一个成功的微调任务，可以从三个核心支柱来进行顶层设计：</p><ul><li><strong>A. 数据策略</strong>：定义了模型“学什么”。这包括如何准备、清洗和组织训练数据，以及如何设定输入输出的长度限制等。</li><li><strong>B. 优化策略</strong>：定义了模型“怎么学”。这涉及到学习率、批次大小、优化器等超参数的选择，它们共同决定了模型参数更新的数学过程。</li><li><strong>C. 训练策略</strong>：定义了“如何保障学习过程顺利进行”。这涉及到训练的步数/轮次、模型的保存与评估机制、以及使用何种分布式或加速技术（如DeepSpeed）等工程层面的配置。</li></ul><h4 id="_3-深度解析-关键超参数与优化器" tabindex="-1"><a class="header-anchor" href="#_3-深度解析-关键超参数与优化器"><span><strong>3. 深度解析：关键超参数与优化器</strong></span></a></h4><h5 id="a-学习率与调度器-learning-rate-scheduler" tabindex="-1"><a class="header-anchor" href="#a-学习率与调度器-learning-rate-scheduler"><span><strong>A. 学习率与调度器 (Learning Rate &amp; Scheduler)</strong></span></a></h5><p>学习率是控制模型每次更新时参数变化幅度的关键参数。</p><ul><li><strong>核心作用</strong>：在调整学习率时，会参考 <code>loss</code> 值。如果训练初期的 <code>loss</code> 收敛太慢，我会适当提高学习率；如果 <code>loss</code> 波动较大，则会降低学习率。一般来说，较低的学习率可以确保模型的稳步调整，避免“学过头”。</li><li><strong>深度拓展：学习率调度器 (Scheduler)</strong> 在实践中，学习率很少保持不变。使用<strong>学习率调度器</strong>是提升训练效果的标配。最经典的策略是 <strong>“预热+余弦衰减”</strong>： <ol><li><strong>预热 (Warmup)</strong>：在训练最开始的几步，使用一个极低的学习率，然后线性地增加到设定的初始值（如 <code>2e-5</code>）。这能帮助模型在训练初期更稳定地适应新数据。</li><li><strong>衰减 (Decay)</strong>：预热期结束后，学习率会按照余弦曲线缓慢下降。这使得模型在训练后期能够以更小的步长进行微调，从而更精细地收敛到最优点。</li></ol></li></ul><h5 id="b-批次大小与梯度累积-batch-size-gradient-accumulation" tabindex="-1"><a class="header-anchor" href="#b-批次大小与梯度累积-batch-size-gradient-accumulation"><span><strong>B. 批次大小与梯度累积 (Batch Size &amp; Gradient Accumulation)</strong></span></a></h5><p>批次大小指的是每次训练“喂”给模型的数据量。</p><ul><li><strong>核心作用</strong>：较小的批次可以帮助模型稳定训练，常见配置为16或32。当算力有限时，调整批次大小尤为重要；如果算力充足，批次越大，训练速度越快。</li><li><strong>深度拓展：有效批次大小与梯度累积</strong> 当显存不足以支持一个理想的大批次时，<strong>梯度累积</strong>是应对的利器。它允许我们通过多次计算小批次的梯度并将其累加，来模拟一次大批次的训练效果。 <ul><li><strong>有效批次大小公式</strong>：<code>Effective Batch Size = per_device_train_batch_size * num_gpus * gradient_accumulation_steps</code></li><li>通过调整 <code>gradient_accumulation_steps</code>，您可以在不增加显存消耗的情况下，实现大批次训练，这有助于稳定训练过程。</li></ul></li></ul><h5 id="c-训练轮次-epochs-vs-max-steps" tabindex="-1"><a class="header-anchor" href="#c-训练轮次-epochs-vs-max-steps"><span><strong>C. 训练轮次 (Epochs vs. Max Steps)</strong></span></a></h5><ul><li><strong>Epoch</strong>：指模型完整遍历一次整个训练数据集的次数。</li><li><strong>Max Steps</strong>：指模型总共执行参数更新的步数。</li><li><strong>选择建议</strong>：对于大小固定的中小型数据集，使用<code>Epochs</code>更直观。对于流式读取或规模极其庞大的数据集，使用<code>Max Steps</code>作为训练的终止条件是更合适的做法。</li></ul><h5 id="d-权重衰减与优化器-weight-decay-optimizer" tabindex="-1"><a class="header-anchor" href="#d-权重衰减与优化器-weight-decay-optimizer"><span><strong>D. 权重衰减与优化器 (Weight Decay &amp; Optimizer)</strong></span></a></h5><ul><li><strong>权重衰减 (Weight Decay)</strong>：这是一种经典的正则化策略，用于防止模型过拟合。它通过对损失函数增加一个与权重大小相关的惩罚项，来限制模型权重变得过大，从而使得模型更简单、泛化能力更强。通常，AdamW优化器的默认值<code>0.01</code>是一个很好的起点，一般无需调整。</li><li><strong>优化器 (Optimizer)</strong>：<code>AdamW</code>是目前LLM微调中最常用、最稳健的优化器。它在传统Adam优化器的基础上，修正了权重衰减的实现方式，使其在现代深度学习任务中效果更好。</li></ul><h4 id="_4-参数高效微调-peft" tabindex="-1"><a class="header-anchor" href="#_4-参数高效微调-peft"><span><strong>4. 参数高效微调 (PEFT)</strong></span></a></h4><p>除了上述传统的全量微调（SFT），为了极大地降低微调对算力的要求，**参数高效微调（PEFT）**已成为当前的主流。</p><ul><li><strong>核心思想</strong>：在微调时，冻结（freeze）预训练模型99.9%以上的参数，只训练极小一部分新增的或指定的参数。</li><li><strong>主流方法：LoRA (Low-Rank Adaptation)</strong><ul><li><strong>原理</strong>：在模型的关键层（如Attention层）旁边，注入两个小型的、可训练的“适配器”矩阵。训练时，所有原始权重都保持不变，只有这些适配器的参数会被更新。</li><li><strong>优势</strong>： <ol><li><strong>显存需求锐减</strong>：使得在单张消费级显卡上微调大模型成为可能。</li><li><strong>训练速度提升</strong>：需要计算和存储的梯度大大减少。</li><li><strong>部署极其灵活</strong>：微调产物只是几十兆（MB）的适配器文件，一个基础模型可以搭配多个不同任务的适配器，实现快速切换。</li></ol></li></ul></li><li><strong>极致优化：QLoRA (Quantized LoRA)</strong><ul><li><strong>原理</strong>：在应用LoRA之前，先将整个冻结的模型权重**量化（Quantize）**成4-bit的超低精度格式，进一步把显存占用压缩到极致。</li><li><strong>效果</strong>：使得在单张24GB显存的显卡上，微调70B（700亿参数）甚至更大规模的模型都成为了现实。</li></ul></li></ul><h4 id="_5-实践-sft训练配置示例与解读" tabindex="-1"><a class="header-anchor" href="#_5-实践-sft训练配置示例与解读"><span><strong>5. 实践：SFT训练配置示例与解读</strong></span></a></h4><p>以下是一个典型的SFT训练配置文件。</p><p>YAML</p><div class="language-plain line-numbers-mode" data-highlighter="shiki" data-ext="plain" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-plain"><span class="line"><span>data_config:</span></span>
<span class="line"><span>  train_file: train.jsonl         # 训练文件路径</span></span>
<span class="line"><span>  val_file: dev.jsonl             # 验证文件路径，用于在训练过程中评估模型性能，防止过拟合</span></span>
<span class="line"><span>  num_proc: 4                     # 数据预处理时使用的并行进程数</span></span>
<span class="line"><span></span></span>
<span class="line"><span>max_input_length: 256             # 输入最大长度，超出部分将被截断</span></span>
<span class="line"><span>max_output_length: 512            # 输出最大长度</span></span>
<span class="line"><span></span></span>
<span class="line"><span>training_args:</span></span>
<span class="line"><span>  output_dir: ./output            # 训练产物（模型检查点、日志等）的保存路径</span></span>
<span class="line"><span>  max_steps: 3000                 # 最大训练步数。与epoch二选一，用于控制训练时长</span></span>
<span class="line"><span>  learning_rate: 2e-5             # 初始学习率，通常会配合学习率调度器使用</span></span>
<span class="line"><span>  per_device_train_batch_size: 1  # 每个GPU的单次训练批次大小</span></span>
<span class="line"><span>  # gradient_accumulation_steps: 8  # 梯度累积步数，有效批次大小为 1 * 卡数 * 8</span></span>
<span class="line"><span>  dataloader_num_workers: 16      # 数据加载时使用的并行线程数，加速数据读取</span></span>
<span class="line"><span>  remove_unused_columns: false    # 是否移除数据集中模型不需要的列，通常保持false</span></span>
<span class="line"><span>  </span></span>
<span class="line"><span>  save_strategy: steps            # 模型保存策略，可设为 &#39;steps&#39; 或 &#39;epoch&#39;</span></span>
<span class="line"><span>  save_steps: 500                 # 每隔500步保存一次模型检查点</span></span>
<span class="line"><span>  </span></span>
<span class="line"><span>  log_level: info                 # 日志级别</span></span>
<span class="line"><span>  logging_strategy: steps         # 日志记录策略</span></span>
<span class="line"><span>  logging_steps: 10               # 每隔10步在控制台打印一次loss等信息</span></span>
<span class="line"><span>  </span></span>
<span class="line"><span>  evaluation_strategy: steps      # 评估策略，与save_strategy配合使用</span></span>
<span class="line"><span>  eval_steps: 500                 # 每隔500步在验证集上进行一次评估</span></span>
<span class="line"><span>  </span></span>
<span class="line"><span>  predict_with_generate: true     # 在评估时使用生成模式，适用于文本生成任务</span></span>
<span class="line"><span>  deepspeed: configs/ds_zero_3.json # 引入DeepSpeed进行分布式训练和内存优化</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_6-结语" tabindex="-1"><a class="header-anchor" href="#_6-结语"><span><strong>6. 结语</strong></span></a></h4><p>微调配置项决定了模型训练的关键行为。常见的调整参数如 <code>learning_rate</code>、<code>max_steps</code>、<code>per_device_train_batch_size</code> 对训练效果和速度都有显著影响。在掌握了PEFT等前沿技术后，我们能以更低的成本进行实验。但最终的“最优解”没有捷径，需要通过大量的、有条理的实验来逐步寻找。</p>`,25)]))}const c=n(l,[["render",t]]),d=JSON.parse('{"path":"/tech/AIGC%E5%BA%94%E7%94%A8%E5%B1%82%E5%BB%BA%E8%AE%BE%E6%80%9D%E8%B7%AF/%E5%BE%AE%E8%B0%83%E7%AD%96%E7%95%A5.html","title":"微调策略","lang":"zh-CN","frontmatter":{"title":"微调策略","date":"2024-06-12T00:00:00.000Z"},"git":{"createdTime":1753758135000,"updatedTime":1753843586000,"contributors":[{"name":"codingXuan","username":"codingXuan","email":"34129858+codingXuan@users.noreply.github.com","commits":3,"url":"https://github.com/codingXuan"}]},"readingTime":{"minutes":6.48,"words":1945},"filePathRelative":"tech/AIGC应用层建设思路/微调策略.md","excerpt":"<h4><strong>1. 微调的目标与定位</strong></h4>\\n<p>在上一篇文章中，我分享了对微调的理解。微调是在预训练模型的基础上，针对垂直领域进行特定任务的训练，或者可以理解为使用自身数据来进一步优化模型，使其更好地适应特定业务场景。这个过程的核心在于通过调整一系列关键参数和策略，来引导模型学习新的知识和行为范式。</p>\\n<h4><strong>2. 微调策略的核心支柱</strong></h4>\\n<p>一个成功的微调任务，可以从三个核心支柱来进行顶层设计：</p>\\n<ul>\\n<li><strong>A. 数据策略</strong>：定义了模型“学什么”。这包括如何准备、清洗和组织训练数据，以及如何设定输入输出的长度限制等。</li>\\n<li><strong>B. 优化策略</strong>：定义了模型“怎么学”。这涉及到学习率、批次大小、优化器等超参数的选择，它们共同决定了模型参数更新的数学过程。</li>\\n<li><strong>C. 训练策略</strong>：定义了“如何保障学习过程顺利进行”。这涉及到训练的步数/轮次、模型的保存与评估机制、以及使用何种分布式或加速技术（如DeepSpeed）等工程层面的配置。</li>\\n</ul>"}');export{c as comp,d as data};
