import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,b as s,o as r}from"./app-7M4PPNja.js";const i={};function g(l,n){return r(),o("div",null,n[0]||(n[0]=[s('<h5 id="_1-训练的目标-为何需要训练" tabindex="-1"><a class="header-anchor" href="#_1-训练的目标-为何需要训练"><span><strong>1. 训练的目标：为何需要训练？</strong></span></a></h5><p>在聊训练策略之前，先要阐述一个概念，为什么要做数据训练？</p><p>通过海量数据让大语言模型（如GLM、GPT等）进行学习，使其能够理解和生成自然语言。这个过程类似于人类通过阅读大量的书籍和文章来学习语言。模型通过数据训练，逐渐掌握语法、语义、上下文理解等语言的各个方面，从而能够在面对新问题时生成有意义的回答或执行特定任务。</p><p>用通俗易懂的说法可以理解为，<strong>人类整理的数据就是大模型的老师</strong>。它的一切知识都来源于训练的数据。</p><h5 id="_2-训练的基石-数据是第一要义" tabindex="-1"><a class="header-anchor" href="#_2-训练的基石-数据是第一要义"><span><strong>2. 训练的基石：数据是第一要义</strong></span></a></h5><p>由此我们可以得到一个结论：<strong>训练数据集的搜集和整理是训练策略中最重要的一步</strong>。当一批垃圾数据与垃圾知识让模型学习之后，得到的模型肯定也是非常糟糕的。因此，高质量的数据是高质量模型的前提。数据准备工作通常包括：</p><ul><li><strong>数据清洗</strong>：去除噪声、无关信息和格式错误。</li><li><strong>数据去重</strong>：移除重复或高度相似的数据，提高训练效率。</li><li><strong>数据格式化</strong>：将数据整理成模型能够理解的、统一的格式（例如，对于指令微调，通常整理成包含<code>instruction</code>, <code>input</code>, <code>output</code>的JSON格式）。</li></ul><h5 id="_3-训练策略的光谱-从预训练到微调" tabindex="-1"><a class="header-anchor" href="#_3-训练策略的光谱-从预训练到微调"><span><strong>3. 训练策略的光谱：从预训练到微调</strong></span></a></h5><p>根据目标和可用资源的不同，训练策略可以大致分为三个层次：</p><ul><li><strong>A. 预训练 (Pre-training)</strong><ul><li><strong>目标</strong>：让模型掌握世界的基础知识和语言的通用规律。</li><li><strong>数据</strong>：海量的、通用的语料库（如整个互联网的文本）。</li><li><strong>场景</strong>：这是模型“从零到一”的过程，需要极其巨大的算力和数据，通常只有大型科技公司或研究机构才会进行。</li></ul></li><li><strong>B. 继续预训练 (Continued Pre-training)</strong><ul><li><strong>目标</strong>：在通用模型的基础上，向其注入特定领域的“知识”，让它“读懂”某个专业领域的“行话”。</li><li><strong>数据</strong>：特定领域的专业文献、书籍、代码库等（如医疗、法律、金融领域的海量文本）。</li><li><strong>场景</strong>：当您希望模型成为某个领域的专家，但又不需要它执行特定任务时使用。</li></ul></li><li><strong>C. 指令微调 (Instruction Fine-tuning / SFT)</strong><ul><li><strong>目标</strong>：在预训练模型的基础上，教会模型“如何做题”，即遵循人类的指令来完成特定的任务。这是<strong>绝大部分应用层开发的核心</strong>。</li><li><strong>数据</strong>：高质量的“指令-回答”数据对（Instruction-Response Pairs）。</li><li><strong>场景</strong>：让模型适应特定任务，如客服对话、代码生成、文案创作等。</li></ul></li></ul><h5 id="_4-训练过程中的核心概念" tabindex="-1"><a class="header-anchor" href="#_4-训练过程中的核心概念"><span><strong>4. 训练过程中的核心概念</strong></span></a></h5><ul><li><strong>A. 损失函数 (Loss Function) 与优化器 (Optimizer)</strong><ul><li><strong>损失函数</strong>：用于衡量模型预测结果与真实结果之间的“差距”或“偏差”的数学公式。这个计算出的偏差值就是我们常说的<strong>loss值</strong>。Loss越小，说明模型预测越贴近真实。</li><li><strong>优化器 (Optimizer)</strong>：是根据loss值来调整模型内部参数的算法。它决定了模型如何“学习”以减小loss。在不具备算法编写能力的情况下，我们依赖于训练框架（如PyTorch）提供的各种优化器支持（如最常用的<code>AdamW</code>），通过试错来找到最佳方案。</li></ul></li><li><strong>B. 周期 (Epoch)、步数 (Step) 与迭代</strong><ul><li>模型的训练是一个迭代的过程，通过多次循环来优化参数。</li><li><strong>Epoch</strong>：代表模型完整学习（遍历）一遍<strong>整个训练数据集</strong>的次数。</li><li><strong>Step</strong>：代表模型进行一次参数更新的动作。每一步，模型会处理一个批次（Batch）的数据。</li><li><strong>关系</strong>：<code>总步数 (Total Steps) = (数据集大小 / 批次大小) * Epochs数量</code>。</li><li><strong>注意</strong>：Epoch并非越多越好。过多的Epoch会导致<strong>过拟合（Overfitting）</strong>——模型过度学习了训练数据的细节，以至于在面对新的、未见过的数据时表现很差。我们通常会监控模型在<strong>验证集</strong>上的表现，一旦性能开始下降，就应**提前停止（Early Stopping）**训练。因此，在尝试初期，将epoch设为较小的值（如1到3）是明智的选择。</li></ul></li></ul><h5 id="_5-结语-微调是核心-试错是方法" tabindex="-1"><a class="header-anchor" href="#_5-结语-微调是核心-试错是方法"><span><strong>5. 结语：微调是核心，试错是方法</strong></span></a></h5><p>在做模型训练时，大部分应用开发者关注的都是<strong>指令微调（SFT）与损失函数优化</strong>这两个方向。在有足够的算力支撑时，可以适当增加训练的轮次（Epochs）和批次大小，来探索模型的性能极限。微调中常用的方法，从需要更新所有参数的<strong>全量微调（FT）</strong>，到只更新极少部分参数的<strong>参数高效微tuning（PEFT，如P-tuning, LoRA等）</strong>，不同的策略支持不同的数据格式和硬件要求。最终，找到最适合自己业务场景的训练策略，需要通过大量的、有条理的实验和试错来完成。</p>',14)]))}const p=t(i,[["render",g]]),c=JSON.parse('{"path":"/AIGC%E5%BA%94%E7%94%A8%E5%B1%82%E5%BB%BA%E8%AE%BE%E6%80%9D%E8%B7%AF/%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5.html","title":"训练策略","lang":"zh-CN","frontmatter":{"title":"训练策略"},"git":{"createdTime":1753758135000,"updatedTime":1753758135000,"contributors":[{"name":"codingXuan","username":"codingXuan","email":"34129858+codingXuan@users.noreply.github.com","commits":1,"url":"https://github.com/codingXuan"}]},"readingTime":{"minutes":4.39,"words":1317},"filePathRelative":"AIGC应用层建设思路/训练策略.md","excerpt":"<h5><strong>1. 训练的目标：为何需要训练？</strong></h5>\\n<p>在聊训练策略之前，先要阐述一个概念，为什么要做数据训练？</p>\\n<p>通过海量数据让大语言模型（如GLM、GPT等）进行学习，使其能够理解和生成自然语言。这个过程类似于人类通过阅读大量的书籍和文章来学习语言。模型通过数据训练，逐渐掌握语法、语义、上下文理解等语言的各个方面，从而能够在面对新问题时生成有意义的回答或执行特定任务。</p>\\n<p>用通俗易懂的说法可以理解为，<strong>人类整理的数据就是大模型的老师</strong>。它的一切知识都来源于训练的数据。</p>\\n<h5><strong>2. 训练的基石：数据是第一要义</strong></h5>"}');export{p as comp,c as data};
