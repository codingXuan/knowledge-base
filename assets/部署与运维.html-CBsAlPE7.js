import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as r,o as s}from"./app-BmWXErMj.js";const l={};function g(i,o){return s(),n("div",null,o[0]||(o[0]=[r("<p><strong>1. 核心原则：将CI/CD扩展到模型和数据</strong></p><p>LLMOps（大语言模型运维）的核心，是将传统DevOps中成熟的自动化、持续集成、持续部署（CI/CD）原则，扩展到包含**代码（Code）、模型（Model）和数据（Data）**三位一体的AI应用生命周期管理中。其目标是实现AI应用快速、可靠、可复现地交付与迭代。</p><p><strong>2. LLMOps的生命周期闭环</strong></p><p>一个完整的LLMOps流程是一个持续的循环：</p><ol><li><strong>数据管理与处理 (CI for Data)</strong>：自动化地收集、清洗、标注新的数据，并进行版本化管理。</li><li><strong>模型训练与微调 (CT for Models)</strong>：当有新数据或新代码时，自动触发模型的微调或训练流程。</li><li><strong>模型评估与验证 (CI for Models)</strong>：使用“黄金评估集”自动评估新生成的模型，只有当新模型的性能指标优于旧模型时，才允许进入部署环节。</li><li><strong>模型打包与部署 (CD for Models)</strong>：将通过验证的模型、相关代码和配置打包成一个可部署的单元（如Docker镜像），并根据预设策略部署到生产环境。</li><li><strong>线上服务与监控</strong>：模型上线后，对其性能、成本、效果和安全性进行全方位监控。</li><li><strong>反馈收集</strong>：从线上监控和用户反馈中收集有价值的数据，并将其送回到第一步的数据管理环节，形成下一次迭代的基础。</li></ol><p><strong>3. LLM应用的部署策略</strong></p><p>在执行部署（CD）环节时，为了保证服务的稳定性和平滑过渡，可以根据业务的重要性和对中断的容忍度，选择不同的部署策略：</p><ul><li><strong>蓝绿部署 (Blue-Green Deployment)</strong>：同时部署两个完全相同的生产环境（蓝色和绿色）。新版本的模型部署在“绿色”环境，经过充分测试后，将所有流量瞬间从“蓝色”环境切换到“绿色”环境。优点是切换速度快，回滚方便；缺点是需要双倍的硬件资源。</li><li><strong>金丝雀发布 (Canary Release)</strong>：先将新版本模型部署到一小部分服务器上，并导入少量用户流量（例如5%）。在确认新版本没有问题后，逐步扩大流量比例，直到100%覆盖。这是目前互联网应用中最主流的平滑发布方式。</li><li><strong>停机维护 (Downtime Deployment)</strong>：对于内部系统或用户容忍度高的场景，最简单的策略是提前发布公告，在业务低峰期（如凌晨）暂停服务，进行升级，完成后再恢复服务。</li></ul><p><strong>4. 关键技术：推理优化与模型服务</strong></p><ul><li><strong>服务框架</strong>：必须使用<strong>vLLM, TGI, Triton</strong>等专为LLM设计的服务框架，它们通过<strong>PagedAttention</strong>和**持续批处理（Continuous Batching）**等技术，能将推理吞吐量提升数倍甚至数十倍。</li><li><strong>硬件与资源管理</strong>： <ul><li><strong>GPU</strong>：LLM推理是典型的计算密集型和显存密集型任务，GPU是必需品。</li><li><strong>容器化</strong>：使用<strong>Docker</strong>将模型、代码和依赖打包，保证环境一致性。</li><li><strong>编排</strong>：使用<strong>Kubernetes</strong>配合GPU设备插件，对GPU资源进行统一的调度和管理。</li></ul></li></ul><p><strong>5. 全面的监控体系</strong></p><p>一个生产级的LLM应用，至少需要监控以下几个维度：</p><ul><li><strong>系统层</strong>：GPU利用率、显存占用、网络I/O、API延迟、QPS。</li><li><strong>成本层</strong>：输入/输出Token数、API调用总成本、每个用户的平均成本。</li><li><strong>效果层</strong>：记录Prompt/Response对，通过用户点赞/点踩、人工抽样或模型自评估来监控服务质量。</li><li><strong>安全层</strong>：监控异常的输入模式，防范Prompt注入和数据泄露风险。</li></ul><p>通过建立这样一套完整的LLMOps体系，我们才能确保AIGC应用在生产环境中不仅能跑起来，还能跑得好、跑得稳，并且能够持续地自我进化。</p>",14)]))}const a=t(l,[["render",g]]),c=JSON.parse('{"path":"/tech/AIGC%E5%BA%94%E7%94%A8%E5%B1%82%E5%BB%BA%E8%AE%BE%E6%80%9D%E8%B7%AF/%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E7%BB%B4.html","title":"部署与运维","lang":"zh-CN","frontmatter":{"title":"部署与运维","date":"2024-10-15T00:00:00.000Z"},"git":{"createdTime":1753758135000,"updatedTime":1753843586000,"contributors":[{"name":"codingXuan","username":"codingXuan","email":"34129858+codingXuan@users.noreply.github.com","commits":3,"url":"https://github.com/codingXuan"}]},"readingTime":{"minutes":3.4,"words":1019},"filePathRelative":"tech/AIGC应用层建设思路/部署与运维.md","excerpt":"<p><strong>1. 核心原则：将CI/CD扩展到模型和数据</strong></p>\\n<p>LLMOps（大语言模型运维）的核心，是将传统DevOps中成熟的自动化、持续集成、持续部署（CI/CD）原则，扩展到包含**代码（Code）、模型（Model）和数据（Data）**三位一体的AI应用生命周期管理中。其目标是实现AI应用快速、可靠、可复现地交付与迭代。</p>\\n<p><strong>2. LLMOps的生命周期闭环</strong></p>\\n<p>一个完整的LLMOps流程是一个持续的循环：</p>\\n<ol>\\n<li><strong>数据管理与处理 (CI for Data)</strong>：自动化地收集、清洗、标注新的数据，并进行版本化管理。</li>\\n<li><strong>模型训练与微调 (CT for Models)</strong>：当有新数据或新代码时，自动触发模型的微调或训练流程。</li>\\n<li><strong>模型评估与验证 (CI for Models)</strong>：使用“黄金评估集”自动评估新生成的模型，只有当新模型的性能指标优于旧模型时，才允许进入部署环节。</li>\\n<li><strong>模型打包与部署 (CD for Models)</strong>：将通过验证的模型、相关代码和配置打包成一个可部署的单元（如Docker镜像），并根据预设策略部署到生产环境。</li>\\n<li><strong>线上服务与监控</strong>：模型上线后，对其性能、成本、效果和安全性进行全方位监控。</li>\\n<li><strong>反馈收集</strong>：从线上监控和用户反馈中收集有价值的数据，并将其送回到第一步的数据管理环节，形成下一次迭代的基础。</li>\\n</ol>"}');export{a as comp,c as data};
