import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,b as l,o}from"./app-Dxd7UrXP.js";const r={};function i(e,n){return o(),t("div",null,n[0]||(n[0]=[l(`<h4 id="_1-核心目标-对齐-alignment-——让模型更像-人" tabindex="-1"><a class="header-anchor" href="#_1-核心目标-对齐-alignment-——让模型更像-人"><span><strong>1. 核心目标：对齐（Alignment）——让模型更像“人”</strong></span></a></h4><p>在完成了**继续预训练（PT，学习知识）<strong>和</strong>指令微调（SFT，学习遵循指令）**之后，模型已经具备了强大的能力。但此时的模型可能还存在一些问题：</p><ul><li>它可能会生成一些无害但无用的“废话”。</li><li>它的回答可能过于“机械”，缺乏真诚和共情。</li><li>在面对模棱两可的问题时，它不知道哪种风格的回答更受人类欢迎。</li></ul><p><strong>RLHF/DPO</strong>的目标，就是解决这个“好不好”的问题。它通过学习人类的<strong>偏好（Preference）数据，将模型的价值观和行为准则，与人类的期望进行对齐（Alignment）</strong>，使其变得更<strong>有用（Helpful）、诚实（Honest）和无害（Harmless）</strong>。</p><h4 id="_2-经典的rlhf三阶段流程" tabindex="-1"><a class="header-anchor" href="#_2-经典的rlhf三阶段流程"><span><strong>2. 经典的RLHF三阶段流程</strong></span></a></h4><p>传统的RLHF是一个复杂的多阶段过程，著名的ChatGPT-3.5就是通过这个流程训练出来的。</p><ul><li><strong>第一阶段：监督微调（SFT）</strong><ul><li>（我们已在上一篇文档中详细讨论）</li><li>这一步的产物是一个已经能听懂指令的基础对话模型。</li></ul></li><li><strong>第二阶段：训练奖励模型（Reward Model, RM）</strong><ul><li><strong>数据准备</strong>：这是RLHF最耗费人力的地方。 <ol><li>让SFT模型对同一个Prompt生成多个不同的回答（例如A, B, C, D）。</li><li>由人工标注员根据一系列标准（如哪个更真实、更友好、更符合指令），对这些回答进行<strong>排序</strong>（例如，<code>D &gt; B &gt; A &gt; C</code>）。</li><li>这样就构成了一份<strong>人类偏好数据集</strong>。</li></ol></li><li><strong>模型训练</strong>： <ul><li>奖励模型（RM）的结构通常与基础模型类似，但其任务不是生成文本，而是<strong>输入一段文本（Prompt+Answer），输出一个单一的分数（Scalar Reward）</strong>。</li><li>我们用偏好数据来训练这个RM，目标是让RM打出的分数，能够尽可能地拟合人类的排序结果（即，<code>RM(D) &gt; RM(B) &gt; RM(A) &gt; RM(C)</code>）。</li><li>训练完成后，我们就拥有了一个可以模仿人类偏好进行打分的“<strong>奖惩模型</strong>”。</li></ul></li></ul></li><li><strong>第三阶段：强化学习（Reinforcement Learning, RL）</strong><ul><li><strong>流程</strong>： <ol><li>将SFT模型作为“策略（Policy）”，让它针对一个新的Prompt生成一个回答。</li><li>将这个“（Prompt，回答）”对送入第二阶段训练好的<strong>奖励模型（RM）</strong>，得到一个奖励分数。</li><li>这个分数被用作<strong>强化学习的“奖励信号”</strong>。</li><li>使用**PPO（Proximal Policy Optimization）**等强化学习算法，根据这个奖励信号，来微调SFT模型的参数。</li></ol></li><li><strong>目标</strong>：不断调整模型，使其生成的回答能够获得越来越高的奖励分数，从而让模型的行为越来越符合人类的偏好。</li></ul></li></ul><h4 id="_3-rlhf的挑战与dpo的兴起" tabindex="-1"><a class="header-anchor" href="#_3-rlhf的挑战与dpo的兴起"><span><strong>3. RLHF的挑战与DPO的兴起</strong></span></a></h4><p>RLHF虽然开创了对齐技术的先河，但其实施起来非常困难：</p><ul><li><strong>流程复杂</strong>：需要训练两个模型（SFT模型和RM），并涉及复杂的强化学习环节。</li><li><strong>训练不稳定</strong>：强化学习的训练过程非常不稳定，对超参数极其敏感。</li><li><strong>资源消耗大</strong>：整个流程需要耗费大量的计算资源。</li></ul><p>为了解决这些问题，**DPO（Direct Preference Optimization，直接偏好优化）**应运而生，并迅速成为当前SOTA（State-of-the-art）模型（如Llama-3, Qwen2）的主流对齐技术。</p><h4 id="_4-dpo-更简单、更直接的偏好对齐" tabindex="-1"><a class="header-anchor" href="#_4-dpo-更简单、更直接的偏好对齐"><span><strong>4. DPO：更简单、更直接的偏好对齐</strong></span></a></h4><p><strong>A. 核心思想</strong> DPO巧妙地证明了，我们可以<strong>跳过“训练奖励模型”和“强化学习”这两个复杂的步骤</strong>，直接在一个步骤内，实现与RLHF同样甚至更好的效果。</p><p><strong>B. DPO的工作流程</strong></p><ol><li><strong>数据准备</strong>：与RLHF类似，同样需要一份人类偏好数据集。但我们不需要完整的排序，只需要成对的比较数据，即对于一个Prompt，哪个回答是**“更优的”（Chosen）<strong>，哪个是</strong>“更差的”（Rejected）**。</li><li><strong>直接优化</strong>：DPO设计了一个特殊的损失函数。在训练时，它会直接对SFT模型进行微调，目标是： <ul><li><strong>增大</strong>模型生成“Chosen”回答的概率。</li><li><strong>减小</strong>模型生成“Rejected”回答的概率。 它将“学习奖励”这一隐式过程，直接转化成了“学习偏好数据”这一监督学习过程。</li></ul></li></ol><p><strong>C. DPO的数据结构</strong> DPO的数据格式非常直观，通常是一个包含三元组的JSONL文件：</p><div class="language-plain line-numbers-mode" data-highlighter="shiki" data-ext="plain" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-plain"><span class="line"><span>{</span></span>
<span class="line"><span>  &quot;prompt&quot;: &quot;请用一句话介绍一下北京。&quot;,</span></span>
<span class="line"><span>  &quot;chosen&quot;: &quot;北京是中国的首都，一座拥有三千年历史的文化名城。&quot;,</span></span>
<span class="line"><span>  &quot;rejected&quot;: &quot;北京，首都。&quot;</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>  &quot;prompt&quot;: &quot;如何学习AI？&quot;,</span></span>
<span class="line"><span>  &quot;chosen&quot;: &quot;学习AI可以从基础的数学和编程知识开始，然后系统地学习机器学习、深度学习等核心课程，并通过实践项目来巩固知识。&quot;,</span></span>
<span class="line"><span>  &quot;rejected&quot;: &quot;学习AI，就去学呗。&quot;</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>D. DPO的优势</strong></p><ul><li><strong>流程简单</strong>：只需在SFT模型上，用偏好数据再进行一次微调即可，无需训练独立的奖励模型，也无需复杂的强化学习。</li><li><strong>训练稳定</strong>：本质上是一次监督学习，比RL训练要稳定得多，更容易实现和调参。</li><li><strong>效果出色</strong>：在多项评测中，DPO被证明可以达到甚至超越传统RLHF的效果。</li></ul><p><strong>总结</strong>：<strong>RLHF</strong>是理解大模型“对齐”思想的<strong>奠基石</strong>，它开创性地引入了人类偏好来指导模型优化。而<strong>DPO</strong>则是这一思想的<strong>现代化、轻量化、高效化的继承者</strong>。在自己的项目中，如果需要进行偏好对齐，<strong>DPO是当前更推荐、更具实践性的技术选择</strong>。它能以更低的成本和更稳定的流程，让模型变得更“聪明”也更“懂事”。</p>`,20)]))}const p=s(r,[["render",i]]),d=JSON.parse('{"path":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB/rlhf_dpo.html","title":"rlhf_dpo","lang":"zh-CN","frontmatter":{"title":"rlhf_dpo"},"git":{"createdTime":1753758135000,"updatedTime":1753758135000,"contributors":[{"name":"codingXuan","username":"codingXuan","email":"34129858+codingXuan@users.noreply.github.com","commits":1,"url":"https://github.com/codingXuan"}]},"readingTime":{"minutes":4.83,"words":1450},"filePathRelative":"数据结构汇总/rlhf_dpo.md","excerpt":"<h4><strong>1. 核心目标：对齐（Alignment）——让模型更像“人”</strong></h4>\\n<p>在完成了**继续预训练（PT，学习知识）<strong>和</strong>指令微调（SFT，学习遵循指令）**之后，模型已经具备了强大的能力。但此时的模型可能还存在一些问题：</p>\\n<ul>\\n<li>它可能会生成一些无害但无用的“废话”。</li>\\n<li>它的回答可能过于“机械”，缺乏真诚和共情。</li>\\n<li>在面对模棱两可的问题时，它不知道哪种风格的回答更受人类欢迎。</li>\\n</ul>\\n<p><strong>RLHF/DPO</strong>的目标，就是解决这个“好不好”的问题。它通过学习人类的<strong>偏好（Preference）数据，将模型的价值观和行为准则，与人类的期望进行对齐（Alignment）</strong>，使其变得更<strong>有用（Helpful）、诚实（Honest）和无害（Harmless）</strong>。</p>"}');export{p as comp,d as data};
